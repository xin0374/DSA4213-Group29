{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed13230e-2e93-4a46-a6b9-421885669dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, uuid, random\n",
    "import os, tempfile, hashlib\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "import string\n",
    "from dataclasses import dataclass\n",
    "from datasets import Dataset\n",
    "from evaluate import load as hf_load\n",
    "from typing import Dict, List, Any, Tuple\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from rank_bm25 import BM25Okapi\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from peft import PeftModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModelForSequenceClassification\n",
    "from typing import Dict, List\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import evaluate\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c75406c-83e7-43f5-b41d-0c122b670645",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def free_cuda():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3350cdb-ed89-40d1-89e7-7d925a85068f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56b5b29d-7b66-46e1-a0e0-cd0189bbf25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42  # any integer you like\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59f012ae-3322-4a93-a6ec-8c7b47522aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c53c217f-073e-42ea-92bf-13c9bc0fc39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in train / val/ test datasets\n",
    "train_df = pd.read_csv(\"medquad_train.csv\")\n",
    "val_df = pd.read_csv(\"medquad_val.csv\")\n",
    "test_df = pd.read_csv(\"medquad_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "047c038d-e7ec-43f5-9eaf-fb256b851dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load any generator (tokenizer + model) by path\n",
    "\n",
    "def load_generator(model_dir: str):\n",
    "    \"\"\"\n",
    "    Loads a generator (tokenizer + CausalLM) from a local directory.\n",
    "    Works for both BASELINE2_DIR (base BioGPT) and FULLMODEL_DIR (merged LoRA).\n",
    "    \"\"\"\n",
    "    tok = AutoTokenizer.from_pretrained(model_dir)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    gen = AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "    gen.config.pad_token_id = tok.pad_token_id\n",
    "    gen = gen.to(device).eval()\n",
    "    return tok, gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1d60497-df76-4608-b3b7-b7e85dccb549",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE2_DIR = \"./biogpt_base\"                    # zero-shot BioGPT (Baseline 2)\n",
    "FULLMODEL_DIR  = \"./finetuned_biogpt\"    # merged LoRA (Full model)\n",
    "\n",
    "tok_base, gen_base = load_generator(BASELINE2_DIR)\n",
    "tok_full, gen_full = load_generator(FULLMODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc8a4466-6d07-43d7-8580-9464fc09260d",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 14.56 GiB of which 16.81 MiB is free. Process 28705 has 3.63 GiB memory in use. Process 46195 has 5.53 GiB memory in use. Process 49664 has 1.50 GiB memory in use. Including non-PyTorch memory, this process has 3.88 GiB memory in use. Of the allocated memory 3.72 GiB is allocated by PyTorch, and 46.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# 1) Dense retriever (Sentence-Transformers + FAISS)\u001b[39;00m\n\u001b[32m     84\u001b[39m \n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# --- 1.1 Load PubMedBERT bi-encoder (MS MARCO tuned) ---\u001b[39;00m\n\u001b[32m     86\u001b[39m DENSE_MODEL = \u001b[33m\"\u001b[39m\u001b[33mpritamdeka/S-PubMedBert-MS-MARCO\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m dense = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDENSE_MODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# --- 1.2 Build dense corpus on the same text used by BM25 ---\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Use the 'doc' (Q + A + meta) or just 'answer'. Start with 'doc' for recall.\u001b[39;00m\n\u001b[32m     91\u001b[39m corpus_texts = train_df[\u001b[33m\"\u001b[39m\u001b[33mdoc\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m).tolist()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/lib/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:367\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[38;5;28mself\u001b[39m.is_hpu_graph_enabled = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.default_prompt_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.default_prompt_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prompts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/lib/python3.13/site-packages/torch/nn/modules/module.py:1371\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1369\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/lib/python3.13/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/lib/python3.13/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 930 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/lib/python3.13/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/lib/python3.13/site-packages/torch/nn/modules/module.py:957\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    958\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/lib/python3.13/site-packages/torch/nn/modules/module.py:1357\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1351\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1352\u001b[39m             device,\n\u001b[32m   1353\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1354\u001b[39m             non_blocking,\n\u001b[32m   1355\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1356\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1363\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 14.56 GiB of which 16.81 MiB is free. Process 28705 has 3.63 GiB memory in use. Process 46195 has 5.53 GiB memory in use. Process 49664 has 1.50 GiB memory in use. Including non-PyTorch memory, this process has 3.88 GiB memory in use. Of the allocated memory 3.72 GiB is allocated by PyTorch, and 46.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# ========= BM25 + PubMedBERT Retrieval ===========\n",
    "# BM25 Baseline (lexical)\n",
    "\n",
    "# Rebuild a richer 'doc' column (question + answer + metadata)\n",
    "def build_doc_row(row):\n",
    "    parts = []\n",
    "    # Put question first so BM25 matches user phrasing better\n",
    "    q = str(row.get(\"question\", \"\")).strip()\n",
    "    a = str(row.get(\"answer\", \"\")).strip()\n",
    "    if q: parts.append(f\"Q: {q}\")\n",
    "    if a: parts.append(f\"A: {a}\")\n",
    "\n",
    "    meta_bits = []\n",
    "    for lab, key in [(\"Entity\", \"entity\"), (\"Type\", \"qtype\"), (\"Source\", \"source\"), (\"URL\", \"url\")]:\n",
    "        val = str(row.get(key, \"\") or \"\").strip()\n",
    "        if val:\n",
    "            meta_bits.append(f\"{lab}: {val}\")\n",
    "    if meta_bits:\n",
    "        parts.append(\"\\n\".join(meta_bits))\n",
    "\n",
    "    return \"\\n\".join(parts).strip()\n",
    "\n",
    "train_df = train_df.copy()\n",
    "train_df[\"doc\"] = train_df.apply(build_doc_row, axis=1)\n",
    "\n",
    "# === Example Result ===\n",
    "# Q: What are the symptoms of pneumonia?\n",
    "# A: The most common symptoms include cough, fever, chills, and difficulty breathing.\n",
    "# Entity: Pneumonia\n",
    "# Source: MedlinePlus\n",
    "# URL: https://medlineplus.gov/pneumonia.html\n",
    "\n",
    "# Build BM25 index over the new 'doc'\n",
    "## strip puncturation and keep medically relevant alphanumeric tokens clean\n",
    "_word_re = re.compile(r\"[A-Za-z0-9]+(?:[-'][A-Za-z0-9]+)?\")\n",
    "\n",
    "def simple_tokenize(text: str):\n",
    "    if not isinstance(text, str): text = \"\" if text is None else str(text)\n",
    "    return _word_re.findall(text.lower())\n",
    "\n",
    "bm25_corpus_texts  = train_df[\"doc\"].tolist()\n",
    "bm25_corpus_tokens = [simple_tokenize(t) for t in bm25_corpus_texts]\n",
    "bm25 = BM25Okapi(bm25_corpus_tokens)\n",
    "_train_rows = train_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# BM25 computes a relevance score for every document in the index, comparing those tokens \n",
    "# against each doc's tokens\n",
    "def bm25_search(query: str, k: int = 20):\n",
    "    q_tokens = simple_tokenize(query)\n",
    "    scores = bm25.get_scores(q_tokens)\n",
    "    if len(scores) == 0:\n",
    "        return train_df.iloc[[]].assign(bm25_score=[])\n",
    "    top_idx = np.argsort(scores)[::-1][:k]\n",
    "    out = _train_rows.iloc[top_idx].copy()\n",
    "    out[\"bm25_score\"] = np.asarray(scores)[top_idx]\n",
    "    return out\n",
    "\n",
    "\n",
    "# --- Simple extractive baseline using the top hit's answer ---\n",
    "def first_sentences(text: str, n: int = 3):\n",
    "    sents = re.split(r'(?<=[.!?])\\s+', (text or \"\").strip())\n",
    "    return \" \".join(sents[:n]).strip()\n",
    "\n",
    "def baseline_bm25_extractive(user_query: str, k_retrieve: int = 5, n_sents: int = 3):\n",
    "    hits = bm25_search(user_query, k=k_retrieve)\n",
    "    if hits.empty:\n",
    "        return {\n",
    "            \"answer\": \"Sorry, I couldnâ€™t find an answer in my knowledge base.\",\n",
    "            \"source\": \"\",\n",
    "            \"url\": \"\",\n",
    "            \"retrieval_candidates\": []\n",
    "        }\n",
    "    top = hits.iloc[0]\n",
    "    extract = first_sentences(top[\"answer\"], n=n_sents) or top[\"answer\"]\n",
    "    return {\n",
    "        \"answer\": extract,\n",
    "        \"source\": str(top.get(\"source\", \"\")),\n",
    "        \"url\": str(top.get(\"url\", \"\")),\n",
    "        \"retrieval_candidates\": hits[[\"question\",\"source\",\"url\",\"bm25_score\"]].to_dict(\"records\")\n",
    "    }\n",
    "\n",
    "# 1) Dense retriever (Sentence-Transformers + FAISS)\n",
    "\n",
    "# --- 1.1 Load PubMedBERT bi-encoder (MS MARCO tuned) ---\n",
    "DENSE_MODEL = \"pritamdeka/S-PubMedBert-MS-MARCO\"\n",
    "dense = SentenceTransformer(DENSE_MODEL, device=device)\n",
    "\n",
    "# --- 1.2 Build dense corpus on the same text used by BM25 ---\n",
    "# Use the 'doc' (Q + A + meta) or just 'answer'. Start with 'doc' for recall.\n",
    "corpus_texts = train_df[\"doc\"].astype(str).tolist()\n",
    "\n",
    "# --- 1.3 Encode corpus (normalized for cosine) ---\n",
    "dense_emb = dense.encode(\n",
    "    corpus_texts, batch_size=128, normalize_embeddings=True,\n",
    "    show_progress_bar=True\n",
    ").astype(\"float32\")\n",
    "\n",
    "# --- 1.4 FAISS index (inner product == cosine due to normalized vectors) ---\n",
    "dim = dense_emb.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(dense_emb)\n",
    "\n",
    "# Row mapping\n",
    "_dense_rows = train_df.reset_index(drop=True)\n",
    "\n",
    "def dense_search(query: str, k: int = 200) -> pd.DataFrame:\n",
    "    qv = dense.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
    "    scores, idx = index.search(qv, k)\n",
    "    idx, scores = idx[0], scores[0]\n",
    "    out = _dense_rows.iloc[idx].copy()\n",
    "    out[\"dense_score\"] = scores\n",
    "    return out\n",
    "\n",
    "# Hybrid retrieval (BM25 + Dense Fusion)\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "def hybrid_search(query: str, k_bm25=200, k_dense=200, k_final=50, alpha=0.5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    alpha = weight for dense (0..1). 0.5 = equal.\n",
    "    \"\"\"\n",
    "    bm = bm25_search(query, k=k_bm25)[[\"question\",\"answer\",\"source\",\"url\",\"bm25_score\"]].copy()\n",
    "    de = dense_search(query, k=k_dense)[[\"question\",\"answer\",\"source\",\"url\",\"dense_score\"]].copy()\n",
    "\n",
    "    merged = bm.merge(de, how=\"outer\", on=[\"question\",\"answer\",\"source\",\"url\"])\n",
    "    merged[\"bm25_score\"] = merged[\"bm25_score\"].fillna(0.0)\n",
    "    merged[\"dense_score\"] = merged[\"dense_score\"].fillna(0.0)\n",
    "\n",
    "    merged[\"bm25_s\"]  = minmax_scale(merged[\"bm25_score\"].to_numpy(dtype=float), copy=True)\n",
    "    merged[\"dense_s\"] = minmax_scale(merged[\"dense_score\"].to_numpy(dtype=float), copy=True)\n",
    "    merged[\"hybrid\"]  = (1 - alpha) * merged[\"bm25_s\"] + alpha * merged[\"dense_s\"]\n",
    "\n",
    "    return merged.sort_values(\"hybrid\", ascending=False).head(k_final).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Cross-encoder re-ranker (PubMedBERT Cross-Encoder)\n",
    "RERANK_NAME = \"ncbi/MedCPT-Cross-Encoder\"  # PubMedBERT cross-encoder\n",
    "ce_tok = AutoTokenizer.from_pretrained(\n",
    "    RERANK_NAME,\n",
    "    model_max_length=512,\n",
    "    truncation_side=\"right\"\n",
    ")\n",
    "ce_model = AutoModelForSequenceClassification.from_pretrained(RERANK_NAME).to(device).eval()\n",
    "\n",
    "def clip_passage(text: str, max_passage_tokens: int = 400) -> str:\n",
    "    # encode *without* specials, clip, decode back\n",
    "    ids = ce_tok.encode(text or \"\", add_special_tokens=False)[:max_passage_tokens]\n",
    "    return ce_tok.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def rerank_cross_encoder(\n",
    "    query: str,\n",
    "    df_candidates: pd.DataFrame,\n",
    "    top_n: int = 8,\n",
    "    batch_size: int = 16,\n",
    "    max_length: int = 512,\n",
    "    max_passage_tokens: int = 400,     # leave room for query + specials\n",
    "):\n",
    "    if df_candidates.empty:\n",
    "        return df_candidates\n",
    "\n",
    "    # Prefer to preserve the *entire query* and truncate only the passage\n",
    "    q = str(query or \"\")\n",
    "    d_texts = [clip_passage(str(a), max_passage_tokens=max_passage_tokens)\n",
    "               for a in df_candidates[\"answer\"].astype(str).tolist()]\n",
    "\n",
    "    scores = []\n",
    "    for i in range(0, len(d_texts), batch_size):\n",
    "        q_batch = [q] * len(d_texts[i:i+batch_size])\n",
    "        d_batch = d_texts[i:i+batch_size]\n",
    "\n",
    "        enc = ce_tok(\n",
    "            q_batch,\n",
    "            d_batch,\n",
    "            padding=\"max_length\",            # ensures uniform shape in a batch\n",
    "            truncation=\"only_second\",        # keep query, truncate passage\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        logits = ce_model(**enc).logits.squeeze(-1)\n",
    "        scores.extend(logits.detach().float().cpu().tolist())\n",
    "\n",
    "    out = df_candidates.copy()\n",
    "    out[\"ce_score\"] = scores\n",
    "    return out.sort_values(\"ce_score\", ascending=False).head(top_n).reset_index(drop=True)\n",
    "\n",
    "# Build context block\n",
    "def build_context_block(reranked_df: pd.DataFrame, max_chars: int = 2200) -> str:\n",
    "    \"\"\"Bullet snippets with source/URL; clipped to max_chars.\"\"\"\n",
    "    ctx = []\n",
    "    total = 0\n",
    "    for _, r in reranked_df.iterrows():\n",
    "        snip = str(r[\"answer\"]).strip()\n",
    "        src  = (r.get(\"source\",\"\") or \"Unknown\").strip()\n",
    "        url  = (r.get(\"url\",\"\") or \"\").strip()\n",
    "        block = f\"- {snip}\\n  (Source: {src} {url})\"\n",
    "        if total + len(block) > max_chars:\n",
    "            break\n",
    "        ctx.append(block); total += len(block)\n",
    "    return \"\\n\".join(ctx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0a6c21-1578-41cf-a840-8a6b5ac24d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########### ORIGINAL ############\n",
    "\n",
    "# # --------------------------------------------------------\n",
    "# # ðŸ”¹ Baseline 1 â€” Finetuned BioGPT (no RAG)\n",
    "# # --------------------------------------------------------\n",
    "# @torch.inference_mode()\n",
    "# def generate_no_rag(user_q: str, tok, gen_model, max_new_tokens: int = 180) -> dict:\n",
    "#     \"\"\"\n",
    "#     Baseline 1: Finetuned BioGPT (no retrieval).\n",
    "#     The model relies only on what it learned during fine-tuning.\n",
    "#     \"\"\"\n",
    "#     system = (\n",
    "#         \"You are a careful medical information assistant for the general public.\\n\"\n",
    "#         \"- Use short sentences and plain language.\\n\"\n",
    "#         \"- Avoid diagnosis; give general guidance and next steps.\\n\"\n",
    "#         \"- If you don't know, say you don't know.\\n\"\n",
    "#     )\n",
    "#     prompt = f\"{system}\\nQuestion: {user_q.strip()}\\nAnswer:\"\n",
    "#     inputs = tok(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "#     out_ids = gen_model.generate(\n",
    "#         **inputs,\n",
    "#         do_sample=False,\n",
    "#         max_new_tokens=max_new_tokens,\n",
    "#         pad_token_id=tok.pad_token_id,\n",
    "#         eos_token_id=tok.eos_token_id,\n",
    "#         use_cache=True\n",
    "#     )\n",
    "\n",
    "#     text = tok.decode(out_ids[0], skip_special_tokens=True)\n",
    "#     ans = text.split(\"Answer:\", 1)\n",
    "#     ans = (ans[1] if len(ans) > 1 else text).strip()\n",
    "#     return {\"answer\": ans, \"evidence\": None}\n",
    "\n",
    "\n",
    "# # --------------------------------------------------------\n",
    "# # ðŸ”¹ Baseline 2 & Full Model â€” BioGPT + RAG\n",
    "# # --------------------------------------------------------\n",
    "# @torch.inference_mode()\n",
    "# def generate_with_rag(\n",
    "#     user_q: str,\n",
    "#     tok, gen_model,\n",
    "#     k_bm25=200, k_dense=200, k_final=50, alpha=0.5,\n",
    "#     top_n=6,\n",
    "#     max_new_tokens=220,\n",
    "#     do_sample=False, top_p=1.0, repetition_penalty=1.02,\n",
    "#     no_repeat_ngram_size=None\n",
    "# ) -> dict:\n",
    "#     cand = hybrid_search(user_q, k_bm25=k_bm25, k_dense=k_dense,\n",
    "#                          k_final=k_final, alpha=alpha)\n",
    "#     rer = rerank_cross_encoder(\n",
    "#         user_q, cand,\n",
    "#         top_n=top_n, batch_size=16, max_length=512, max_passage_tokens=400\n",
    "#     )\n",
    "#     prompt = build_rag_prompt(user_q, rer, tok=tok, ctx_token_budget=750)\n",
    "\n",
    "#     inputs = tok(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "#     gen_kwargs = dict(\n",
    "#         do_sample=do_sample,\n",
    "#         top_p=top_p,\n",
    "#         max_new_tokens=max_new_tokens,\n",
    "#         repetition_penalty=repetition_penalty,\n",
    "#         pad_token_id=tok.pad_token_id,\n",
    "#         eos_token_id=tok.eos_token_id,\n",
    "#         use_cache=True\n",
    "#     )\n",
    "#     if no_repeat_ngram_size is not None:\n",
    "#         gen_kwargs[\"no_repeat_ngram_size\"] = no_repeat_ngram_size\n",
    "\n",
    "#     out_ids = gen_model.generate(**inputs, **gen_kwargs)\n",
    "#     text = tok.decode(out_ids[0], skip_special_tokens=True)\n",
    "#     ans = text.split(\"Answer:\", 1)\n",
    "#     ans = (ans[1] if len(ans) > 1 else text).strip()\n",
    "\n",
    "#     cols = [c for c in [\"answer\",\"source\",\"url\",\"ce_score\"] if c in rer.columns]\n",
    "#     used = rer[cols].copy() if cols else pd.DataFrame(columns=[\"answer\",\"source\",\"url\",\"ce_score\"])\n",
    "\n",
    "#     return {\"answer\": ans, \"evidence\": used}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100e9424-fb71-4426-b93a-33f327c77b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Finalized Generation Pipelines\n",
    "# =============================\n",
    "\n",
    "\n",
    "# Detect device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# ====== No RAG (Baseline 1) ======\n",
    "@torch.inference_mode()\n",
    "def generate_no_rag(user_q: str, tok, gen_model,\n",
    "                    max_new_tokens: int = 150,\n",
    "                    repetition_penalty: float = 1.1,\n",
    "                    no_repeat_ngram_size: int = 3) -> dict:\n",
    "    \"\"\"\n",
    "    Baseline 1: Finetuned BioGPT (no retrieval).\n",
    "    The model relies only on what it learned during fine-tuning.\n",
    "    \"\"\"\n",
    "    system = (\n",
    "        \"You are a careful medical information assistant for the general public.\\n\"\n",
    "        \"- Use short sentences and plain language.\\n\"\n",
    "        \"- Avoid diagnosis; give general guidance and next steps.\\n\"\n",
    "        \"- If you don't know, say you don't know.\\n\"\n",
    "    )\n",
    "    prompt = f\"{system}\\nQuestion: {user_q.strip()}\\nAnswer:\"\n",
    "\n",
    "    # Tokenize safely\n",
    "    inputs = tok(prompt, return_tensors=\"pt\",\n",
    "                 truncation=True, max_length=512).to(DEVICE)\n",
    "\n",
    "    out_ids = gen_model.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,  # greedy decoding\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        pad_token_id=tok.pad_token_id,\n",
    "        eos_token_id=tok.eos_token_id,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    text = tok.decode(out_ids[0], skip_special_tokens=True)\n",
    "    ans = text.split(\"Answer:\", 1)\n",
    "    ans = (ans[1] if len(ans) > 1 else text).strip()\n",
    "    return {\"answer\": ans, \"evidence\": None}\n",
    "\n",
    "\n",
    "# ====== RAG (Baseline 2 + Full Model) ======\n",
    "SYSTEM_RAG_FINE = (\n",
    "    \"You are a careful medical information assistant for the general public.\\n\"\n",
    "    \"- Answer ONLY using the Context bullets; if missing, say you don't know.\\n\"\n",
    "    \"- Use short sentences and plain language. Avoid diagnosis; give general guidance and next steps.\\n\"\n",
    "    \"- Add bracketed numeric citations [1], [2] that refer to the bullets you used.\\n\"\n",
    ")\n",
    "\n",
    "SYSTEM_RAG_BASE = (\n",
    "    \"Answer the following medical question briefly using only the context below. \"\n",
    "    \"If the context does not contain the answer, reply: 'I don't know.'\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "def build_rag_prompt(user_q: str,\n",
    "                     reranked_df: pd.DataFrame,\n",
    "                     tok,\n",
    "                     ctx_token_budget: int = 700,\n",
    "                     system_text: str = SYSTEM_RAG_FINE) -> str:\n",
    "    \"\"\"\n",
    "    Build a concise RAG prompt with a safe token budget.\n",
    "    \"\"\"\n",
    "    bullets, used = [], 0\n",
    "    for i, (_, r) in enumerate(reranked_df.iterrows(), start=1):\n",
    "        block = f\"[{i}] {str(r['answer']).strip()}\\n\" \\\n",
    "                f\"(Source: {(r.get('source') or 'Unknown').strip()} {(r.get('url') or '').strip()})\"\n",
    "        ids = tok.encode(block, add_special_tokens=False)\n",
    "        if used + len(ids) > ctx_token_budget:\n",
    "            break\n",
    "        bullets.append(block)\n",
    "        used += len(ids)\n",
    "\n",
    "    ctx = \"\\n\".join(bullets)\n",
    "    return (\n",
    "        f\"{system_text}\\n\"\n",
    "        f\"Context:\\n{ctx}\\n\\n\"\n",
    "        f\"Question: {user_q.strip()}\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_with_rag(\n",
    "    user_q: str,\n",
    "    tok,\n",
    "    gen_model,\n",
    "    use_base_prompt: bool = False,  # True if using BioGPT-base (zero-shot)\n",
    "    k_bm25=200, k_dense=200, k_final=50, alpha=0.5,\n",
    "    top_n=6,\n",
    "    max_new_tokens=150,\n",
    "    do_sample=False,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.1,\n",
    "    no_repeat_ngram_size=3\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Full RAG pipeline for Baseline 2 or Full Model.\n",
    "    Returns: {\"answer\": str, \"evidence\": pd.DataFrame[answer, source, url, ce_score]}\n",
    "    \"\"\"\n",
    "    # --- 1. Hybrid retrieval ---\n",
    "    cand = hybrid_search(user_q, k_bm25=k_bm25,\n",
    "                         k_dense=k_dense, k_final=k_final, alpha=alpha)\n",
    "\n",
    "    # --- 2. Cross-encoder re-ranking ---\n",
    "    rer = rerank_cross_encoder(\n",
    "        user_q, cand, top_n=top_n,\n",
    "        batch_size=16, max_length=512, max_passage_tokens=400\n",
    "    )\n",
    "\n",
    "    # --- 3. Build RAG prompt ---\n",
    "    system_text = SYSTEM_RAG_BASE if use_base_prompt else SYSTEM_RAG_FINE\n",
    "    prompt = build_rag_prompt(user_q, rer, tok=tok,\n",
    "                              ctx_token_budget=700, system_text=system_text)\n",
    "\n",
    "    # --- 4. Generate ---\n",
    "    inputs = tok(prompt, return_tensors=\"pt\",\n",
    "                 truncation=True, max_length=512).to(DEVICE)\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        do_sample=do_sample,\n",
    "        top_p=top_p,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        pad_token_id=tok.pad_token_id,\n",
    "        eos_token_id=tok.eos_token_id,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    out_ids = gen_model.generate(**inputs, **gen_kwargs)\n",
    "    text = tok.decode(out_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    ans = text.split(\"Answer:\", 1)\n",
    "    ans = (ans[1] if len(ans) > 1 else text).strip()\n",
    "\n",
    "    # --- 5. Extract evidence ---\n",
    "    cols = [c for c in [\"answer\", \"source\", \"url\", \"ce_score\"] if c in rer.columns]\n",
    "    used = rer[cols].copy() if cols else pd.DataFrame(\n",
    "        columns=[\"answer\", \"source\", \"url\", \"ce_score\"]\n",
    "    )\n",
    "\n",
    "    return {\"answer\": ans, \"evidence\": used}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81875a29-633e-4456-a4da-46600b1b06ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")          # ROUGE-1/2/L\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "smooth = SmoothingFunction().method3    # BLEU smoothing for short answers\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = s.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "def exact_match(pred: str, gold: str) -> int:\n",
    "    return int(normalize_text(pred) == normalize_text(gold))\n",
    "\n",
    "def token_f1(pred: str, gold: str) -> float:\n",
    "    pt = normalize_text(pred).split()\n",
    "    gt = normalize_text(gold).split()\n",
    "    common = set(pt) & set(gt)\n",
    "    num_same = sum(min(pt.count(w), gt.count(w)) for w in common)\n",
    "    if not pt or not gt:\n",
    "        return float(pt == gt)\n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "    precision = num_same / len(pt)\n",
    "    recall    = num_same / len(gt)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def bleu_score(pred: str, gold: str) -> float:\n",
    "    # sentence_bleu expects list of references (each is list of tokens)\n",
    "    ref = normalize_text(gold).split()\n",
    "    hyp = normalize_text(pred).split()\n",
    "    if not hyp:\n",
    "        return 0.0\n",
    "    return sentence_bleu([ref], hyp, smoothing_function=smooth)\n",
    "\n",
    "def rouge_l(preds, refs) -> float:\n",
    "    # returns ROUGE-L fmeasure\n",
    "    out = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "    return float(out[\"rougeL\"])\n",
    "\n",
    "\n",
    "def content_tokens(s: str):\n",
    "    toks = normalize_text(s).split()\n",
    "    return [t for t in toks if (t not in stop_words) and (t.isalpha())]\n",
    "\n",
    "def support_ratio(pred: str, contexts: list[str]) -> float:\n",
    "    pred_ct = content_tokens(pred)\n",
    "    if not pred_ct:\n",
    "        return 1.0  # empty prediction = not hallucinated\n",
    "    ctx = \" \".join(contexts)\n",
    "    ctx_set = set(content_tokens(ctx))\n",
    "    supported = sum(1 for t in pred_ct if t in ctx_set)\n",
    "    return supported / max(1, len(pred_ct))\n",
    "\n",
    "def hallucinated(pred: str, contexts: list[str], thresh: float = 0.6) -> int:\n",
    "    return int(support_ratio(pred, contexts) < thresh)\n",
    "\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "def bert_score(preds, refs, lang=\"en\", model_type=\"bert-base-uncased\"):\n",
    "    \"\"\"\n",
    "    preds: list[str] of candidate answers\n",
    "    refs:  list[str] of reference answers\n",
    "    returns: dict with keys 'precision', 'recall', 'f1'\n",
    "    \"\"\"\n",
    "    result = bertscore.compute(\n",
    "        predictions=preds,\n",
    "        references=refs,\n",
    "        lang=lang,\n",
    "        model_type=model_type\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9501d89-fa11-4b74-8237-47f4c4ef8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Unified evaluator for Baselines & Full Model (fixed) ---\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    eval_df: pd.DataFrame,\n",
    "    generation_fn,                          # <-- function comes 2nd now\n",
    "    name: str,\n",
    "    save_path: str = None\n",
    ") -> Tuple[Dict, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Evaluate a generator function over a validation dataframe.\n",
    "    generation_fn: callable(q) -> {\"answer\": str, \"evidence\": pd.DataFrame or None}\n",
    "    Returns: (metrics_dict, detailed_df)\n",
    "    \"\"\"\n",
    "    preds, refs = [], []\n",
    "    em_list, f1_list, bleu_list = [], [], []\n",
    "    supp_list, hall_list = [], []\n",
    "    rows_out = []\n",
    "\n",
    "    print(f\"\\n=== Evaluating {name} on {len(eval_df)} samples ===\")\n",
    "\n",
    "    for _, row in tqdm(eval_df.iterrows(), total=len(eval_df), desc=f\"Eval {name}\"):\n",
    "        q = str(row[\"question\"])\n",
    "        gold = str(row[\"answer\"])\n",
    "\n",
    "        # --- Generate ---\n",
    "        try:\n",
    "            out = generation_fn(q)\n",
    "            if not isinstance(out, dict):\n",
    "                raise TypeError(f\"generator_fn returned {type(out)}, expected dict\")\n",
    "            pred = str(out.get(\"answer\", \"\") or \"\").strip()\n",
    "            ev_df = out.get(\"evidence\", None)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Generation failed for: {q[:60]}... ({e})\")\n",
    "            pred, ev_df = \"\", None\n",
    "\n",
    "        # --- Context from retrieved evidence (if any) ---\n",
    "        ctxs = []\n",
    "        if ev_df is not None:\n",
    "            try:\n",
    "                if hasattr(ev_df, \"empty\") and not ev_df.empty and \"answer\" in ev_df.columns:\n",
    "                    ctxs = ev_df[\"answer\"].astype(str).tolist()\n",
    "            except Exception:\n",
    "                ctxs = []\n",
    "\n",
    "        # --- Metrics ---\n",
    "        preds.append(pred)\n",
    "        refs.append(gold)\n",
    "        try:\n",
    "            em_list.append(exact_match(pred, gold))\n",
    "        except Exception:\n",
    "            em_list.append(0.0)\n",
    "        try:\n",
    "            f1_list.append(token_f1(pred, gold))\n",
    "        except Exception:\n",
    "            f1_list.append(0.0)\n",
    "        try:\n",
    "            bleu_list.append(bleu_score(pred, gold))\n",
    "        except Exception:\n",
    "            bleu_list.append(0.0)\n",
    "        try:\n",
    "            supp_list.append(support_ratio(pred, ctxs))\n",
    "        except Exception:\n",
    "            supp_list.append(0.0)\n",
    "        try:\n",
    "            hall_list.append(hallucinated(pred, ctxs, thresh=0.6))\n",
    "        except Exception:\n",
    "            hall_list.append(1.0)  # pessimistic default if check fails\n",
    "\n",
    "        # --- Record per sample (safe evidence extraction) ---\n",
    "        def _safe_ev(i):\n",
    "            try:\n",
    "                return ev_df.iloc[i][\"answer\"] if ev_df is not None and len(ev_df) > i and \"answer\" in ev_df.columns else \"\"\n",
    "            except Exception:\n",
    "                return \"\"\n",
    "\n",
    "        rows_out.append({\n",
    "            \"question\": q,\n",
    "            \"gold_answer\": gold,\n",
    "            \"prediction\": pred,\n",
    "            \"EM\": em_list[-1],\n",
    "            \"F1\": f1_list[-1],\n",
    "            \"BLEU\": bleu_list[-1],\n",
    "            \"SupportRatio\": supp_list[-1],\n",
    "            \"Hallucinated\": hall_list[-1],\n",
    "            \"evidence_1\": _safe_ev(0),\n",
    "            \"evidence_2\": _safe_ev(1),\n",
    "            \"evidence_3\": _safe_ev(2),\n",
    "        })\n",
    "\n",
    "    # --- Aggregate Metrics ---\n",
    "    try:\n",
    "        rouge_l_f = rouge_l(preds, refs)\n",
    "    except Exception:\n",
    "        rouge_l_f = 0.0\n",
    "\n",
    "    # BERTScore (skip if all preds empty to avoid noisy warnings)\n",
    "    try:\n",
    "        any_pred_text = any(bool(p.strip()) for p in preds)\n",
    "        if any_pred_text:\n",
    "            bert_res = bert_score(preds, refs)\n",
    "            bert_p = float(np.mean(bert_res.get(\"precision\", [0.0])))\n",
    "            bert_r = float(np.mean(bert_res.get(\"recall\", [0.0])))\n",
    "            bert_f = float(np.mean(bert_res.get(\"f1\", [0.0])))\n",
    "        else:\n",
    "            bert_p = bert_r = bert_f = 0.0\n",
    "    except Exception:\n",
    "        bert_p = bert_r = bert_f = 0.0\n",
    "\n",
    "    results = {\n",
    "        \"N\": len(eval_df),\n",
    "        \"ExactMatch\": float(np.mean(em_list)) if em_list else 0.0,\n",
    "        \"TokenF1\": float(np.mean(f1_list)) if f1_list else 0.0,\n",
    "        \"BLEU\": float(np.mean(bleu_list)) if bleu_list else 0.0,\n",
    "        \"ROUGE-L\": float(rouge_l_f),\n",
    "        \"SupportRatio(avg)\": float(np.mean(supp_list)) if supp_list else 0.0,\n",
    "        \"HallucinationRate(<0.6 support)\": float(np.mean(hall_list)) if hall_list else 0.0,\n",
    "        \"BERTScore_P\": bert_p,\n",
    "        \"BERTScore_R\": bert_r,\n",
    "        \"BERTScore_F1\": bert_f,\n",
    "    }\n",
    "\n",
    "    # --- Save optional CSV ---\n",
    "    details = pd.DataFrame(rows_out)\n",
    "    if save_path:\n",
    "        try:\n",
    "            details.to_csv(save_path, index=False)\n",
    "            print(f\"Saved per-sample details to {save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to save CSV to {save_path}: {e}\")\n",
    "\n",
    "    print(f\"\\n=== {name} Summary ===\")\n",
    "    for k, v in results.items():\n",
    "        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "    return results, details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08096898-cc2d-4dde-aa0e-cae39cd0968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS\n",
    "\n",
    "# --- Baseline 1 (fine-tuned BioGPT only) ---\n",
    "gen_fn_baseline1 = lambda q: generate_no_rag(q, tok_full, gen_full)\n",
    "\n",
    "# --- Baseline 2 (RAG + zero-shot BioGPT) ---\n",
    "gen_fn_baseline2 = lambda q: generate_with_rag(q, tok_base, gen_base)\n",
    "\n",
    "# --- Full Model (RAG + fine-tuned BioGPT) ---\n",
    "gen_fn_full = lambda q: generate_with_rag(q, tok_full, gen_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09575abb-6ad6-402c-adb8-891f0b2fbc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline1_results = evaluate_model(val_df, gen_fn_baseline1, \"Baseline 1 â€“ Finetuned BioGPT\", \"baseline1_val.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba82a7-ccd6-47d5-9b43-995b99cc43fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline2_results = evaluate_model(val_df, gen_fn_baseline2, \"Baseline 2 â€“ RAG + Base BioGPT\", \"baseline2_val.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03e0b0c-aef3-45b5-9a15-828b2d4a416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fullmodel_results  = evaluate_model(val_df, gen_fn_full, \"Full Model â€“ RAG + Finetuned BioGPT\", \"fullmodel_val.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73196e6f-b68a-4e09-9073-6af8aee48fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline1_results, baseline1_details = baseline1_results\n",
    "baseline2_results, baseline2_details = baseline2_results\n",
    "fullmodel_results, fullmodel_details = fullmodel_results\n",
    "\n",
    "summary_df = pd.DataFrame([\n",
    "    {\"Model\": \"Baseline 1 (Fine-tuned only)\", **baseline1_results},\n",
    "    {\"Model\": \"Baseline 2 (RAG + Base BioGPT)\", **baseline2_results},\n",
    "    {\"Model\": \"Full Model (RAG + Fine-tuned)\", **fullmodel_results}\n",
    "])\n",
    "\n",
    "# Drop the â€œNâ€ column safely if it exists\n",
    "summary_df.drop(columns=[\"N\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Save to CSV\n",
    "summary_df.to_csv(\"model_comparison_summary.csv\", index=False)\n",
    "\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a0e0dc-e66a-4782-abb5-4f338844c5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
